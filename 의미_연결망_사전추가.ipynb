{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "의미 연결망_사전추가.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yangyeji2020311166/ML/blob/main/%EC%9D%98%EB%AF%B8_%EC%97%B0%EA%B2%B0%EB%A7%9D_%EC%82%AC%EC%A0%84%EC%B6%94%EA%B0%80.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hwa_hztlWj66"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8BgiFviXx3L"
      },
      "source": [
        "!apt-get update \n",
        "!apt-get install g++ openjdk-8-jdk python-dev python3-dev \n",
        "!pip3 install JPype1-py3 \n",
        "!pip3 install konlpy \n",
        "!JAVA_HOME=\"C:\\Program Files\\Java\\jdk-17\\\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f=open('/content/스마트폰 크롤링.txt','r',encoding='utf-8')\n",
        "txt = f.readlines()\n",
        "f.close()\n",
        "\n",
        "txt2=[]          #중복 댓글 제거\n",
        "for item in txt:\n",
        "    if item not in txt2:\n",
        "        txt2.append(item)\n",
        "\n",
        "\n",
        "print(txt2)"
      ],
      "metadata": {
        "id": "1bl_x0T_4lYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##대댓글 아이디 삭제하기\n",
        "remove_id_list=[]\n",
        "for i in range(len(txt2)):\n",
        "  if '?@' in txt2[i]:\n",
        "    cnt = 0\n",
        "    for j in range(len(txt2[i])):\n",
        "      if (txt2[i][j] == '?' or txt2[i][j] == ' '):\n",
        "        if (txt2[i][j] == '?'):\n",
        "          cnt+=1\n",
        "      elif cnt>=2:\n",
        "        break\n",
        "    tmp = txt2[i].split(sep='?',maxsplit=cnt)\n",
        "    remove_id_list.append(tmp[cnt])\n",
        "  else:\n",
        "    remove_id_list.append(txt2[i])\n",
        "\n",
        "print(remove_id_list)"
      ],
      "metadata": {
        "id": "dq3fX-vm4oaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/lovit/customized_konlpy.git\n",
        "!pip install customized_konlpy"
      ],
      "metadata": {
        "id": "2OG4DeoNLZCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ckonlpy.tag import Twitter\n",
        "\n",
        "twitter=Twitter()\n",
        "twitter.add_dictionary(['유튜버','카툭튀','인트로','잇섭','플래그십','불닭','불닭볶음탕','불닭탕','호불호','오뚜기','컵누들','새우탕','칼국수','콕콕콕','불닭볶음면','까르보불닭','광천김라면','까르보나라','참치마요','치즈볶이','민생라면',\n",
        "                        '장인라면','미역국라면','왕뚜껑','틈새라면','콩고기','진짬뽕','맛짬뽕','네넴띤','국민비빔면','비빔불닭면','칼빔면','정비빔면','배홍동비빔면',\n",
        "                        '제주해물맛라면','팔도라면','호빵맨컵라면','튀김우동','제주해녀라면','독도사랑라면','삼양라면','불닭게티','꼬꼬면','돈코츠라멘','배홍동'], 'Noun')\n"
      ],
      "metadata": {
        "id": "slFnrLTbLZgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "import konlpy\n",
        "from collections import Counter\n",
        "\n",
        "for i, document in enumerate(remove_id_list):\n",
        "    clean_words = []\n",
        "    for word in twitter.pos(document, stem=True): #어간 추출\n",
        "        if word[1] in ['Noun']: #이부분에 원하는 품사 넣기\n",
        "            clean_words.append(word[0])\n",
        "    remove_id_list[i] = clean_words\n",
        "\n",
        "print(remove_id_list)"
      ],
      "metadata": {
        "id": "WFnUXkI54pEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#불용어 제거\n",
        "f = open('/content/스탑워드.txt', \"rt\", encoding=\"utf-8\") \n",
        "lines = f.readlines()\n",
        "stopwords = []\n",
        "unique_Noun_words=[]\n",
        "for line in lines:\n",
        "    line = line.replace('\\n', '')\n",
        "    stopwords.append(line)\n",
        "f.close()\n",
        "print(stopwords)"
      ],
      "metadata": {
        "id": "vcJYGN8-9C0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_Noun_words=[]\n",
        "for i in range(len(remove_id_list)):#이차원 배열 전체에서 for 문 돌리기\n",
        "  tmp=[]\n",
        "  for j in range(len(remove_id_list[i])):#배열 안에서 일차원 배열의 길이가 서로 다 다르니깐 그 길이만큼 돌리면서\n",
        "    if remove_id_list[i][j] not in stopwords:#불용어 있는지 검사 없으면 tmp에 임시저장\n",
        "      tmp.append(remove_id_list[i][j])\n",
        "  unique_Noun_words.append(tmp)#tmp에 임시저장한 리스트를 배열 전체 하나가 끝날때마다 검사\n",
        "#for i in range(len(unique_Noun_words)):\n",
        "  #print(unique_Noun_words[i])"
      ],
      "metadata": {
        "id": "euTGHbVt9uB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(unique_Noun_words)"
      ],
      "metadata": {
        "id": "K5BLCuwtw6J8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bE4ik679hfoN"
      },
      "source": [
        "!pip3 install apyori"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from mlxtend.frequent_patterns import apriori\n",
        "\n",
        "te = TransactionEncoder()\n",
        "te_result = te.fit(unique_Noun_words).transform(unique_Noun_words)\n",
        "df2 = pd.DataFrame(te_result, columns = te.columns_)\n",
        "df2"
      ],
      "metadata": {
        "id": "u9ADSC2Zxxxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_list = df2.columns\n",
        "word_length = len(column_list)\n",
        "\n",
        "from tqdm import notebook\n",
        "import time\n",
        "# 각 단어쌍의 빈도수를 저장할 dictionary 생성\n",
        "count_dict = {}\n",
        "\n",
        "for doc_number in notebook.tqdm(range(len(df2)), desc='단어쌍 만들기 진행중'):\n",
        "    tmp = df2.loc[doc_number]           # 현재 문서의 단어 출현 빈도 데이터를 가져온다.\n",
        "    for i, word1 in enumerate(column_list):\n",
        "        if tmp[word1]:              # 현재 문서에 첫번째 단어가 존재할 경우\n",
        "            for j in range(i + 1, word_length):\n",
        "                if tmp[column_list[j]]:              # 현재 문서에 두번째 단어가 존재할 경우\n",
        "                    count_dict[column_list[i], column_list[j]] = count_dict.get((column_list[i], column_list[j]), 0) + max(tmp[word1], tmp[column_list[j]])\n",
        "\n",
        "# count_list에 word1, word2, frequency 형태로 저장할 것이다.\n",
        "count_list = []\n",
        "\n",
        "for words in count_dict:\n",
        "    count_list.append([words[0], words[1], count_dict[words]])\n",
        "\n",
        "# 단어쌍 동시 출현 빈도를 DataFrame 형식으로 만든다.\n",
        "df3 = pd.DataFrame(count_list, columns=[\"word1\", \"word2\", \"freq\"])\n",
        "df3 = df3.sort_values(by=['freq'], ascending=False)\n",
        "df3 = df3.reset_index(drop=True)\n",
        "df3"
      ],
      "metadata": {
        "id": "c7QY8K6vx31q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####불용어 제거 부분\n",
        "del df2['되다']\n",
        "del df2['이다']\n",
        "del df2['쓰다']\n",
        "del df2['보다']\n",
        "del df2['하다']\n",
        "#######\n",
        "\n",
        "result = apriori(df2, min_support=0.01, use_colnames=True)\n",
        "\n",
        "result['length'] = result['itemsets'].apply(lambda x: len(x))\n",
        "result = result[(result['length']==2) & (result['support']>= 0.01)].sort_values(by='support',ascending=False)\n",
        "result.head(20)"
      ],
      "metadata": {
        "id": "8OOU82TjzBc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhDtLxICcwdT"
      },
      "source": [
        "#연관규칙\n",
        "#원래 작성했던 부분\n",
        "#from apyori import apriori\n",
        "#result = (list(apriori(unique_Noun_words, min_support = 0.01)))\n",
        "#df = pd.DataFrame(result)\n",
        "#df['length'] = df['items'].apply(lambda x: len(x))\n",
        "#df = df[(df['length']==2) & (df['support']>= 0.01)].sort_values(by='support',ascending=False)\n",
        "#df.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEhK0U5Kehld"
      },
      "source": [
        "G = nx.Graph()\n",
        "ar=(result['items']); G.add_edges_from(ar)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9WkaoOriFRn"
      },
      "source": [
        "pr = nx.pagerank(G)\n",
        "nsize = np.array([v for v in pr.values()])\n",
        "nsize = 2000 * (nsize - min(nsize)) / (max(nsize) - min(nsize))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5Wcr3bOip5S"
      },
      "source": [
        "pos = nx.random_layout(G)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pj1gvADflQ3H"
      },
      "source": [
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xu2LJ-ali5MZ"
      },
      "source": [
        "plt.figure(figsize=(16,12));plt.axis('off')\n",
        "nx.draw_networkx(G, font_family='NanumBarunGothic',font_size=16,\n",
        "                 pos=pos, node_color=list(pr.values()), node_size=nsize,\n",
        "                 alpha=0.7, edge_color='.5',cmap=plt.cm.YlGn)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}