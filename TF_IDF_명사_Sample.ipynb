{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF - IDF 명사 Sample.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMIgcsqIqLVhJ8GWJz4x2sO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yangyeji2020311166/ML/blob/main/TF_IDF_%EB%AA%85%EC%82%AC_Sample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMs8k3f652D9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "from future.utils import iteritems\n",
        "from collections import Counter\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update \n",
        "!apt-get install g++ openjdk-8-jdk python-dev python3-dev \n",
        "!pip3 install JPype1-py3 \n",
        "!pip3 install konlpy \n",
        "!JAVA_HOME=\"C:\\Program Files\\Java\\jdk-17\\\""
      ],
      "metadata": {
        "id": "t0qBovV5574M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f=open('/content/스마트폰 크롤링.txt','r',encoding='utf-8')\n",
        "txt = f.readlines()\n",
        "f.close()\n",
        "\n",
        "txt2=[]          #중복 댓글 제거\n",
        "for item in txt:\n",
        "    if item not in txt2:\n",
        "        txt2.append(item)\n",
        "\n",
        "\n",
        "print(txt2)"
      ],
      "metadata": {
        "id": "rU6AxJRO572P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##대댓글 아이디 삭제하기\n",
        "remove_id_list=[]\n",
        "for i in range(len(txt2)):\n",
        "  if '?@' in txt2[i]:\n",
        "    cnt = 0\n",
        "    for j in range(len(txt2[i])):\n",
        "      if (txt2[i][j] == '?' or txt2[i][j] == ' '):\n",
        "        if (txt2[i][j] == '?'):\n",
        "          cnt+=1\n",
        "      elif cnt>=2:\n",
        "        break\n",
        "    tmp = txt2[i].split(sep='?',maxsplit=cnt)\n",
        "    remove_id_list.append(tmp[cnt])\n",
        "  else:\n",
        "    remove_id_list.append(txt2[i])\n",
        "\n",
        "print(remove_id_list)"
      ],
      "metadata": {
        "id": "lxWeuHqY570h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "import konlpy\n",
        "okt = konlpy.tag.Okt()\n",
        "\n",
        "for i, document in enumerate(remove_id_list):\n",
        "    okt = konlpy.tag.Okt()\n",
        "    clean_words = []\n",
        "    for word in okt.pos(document, stem=True): #어간 추출\n",
        "        if word[1] in ['Noun']: #명사\n",
        "            clean_words.append(word[0])\n",
        "    print(clean_words) \n",
        "    remove_id_list[i] = clean_words\n",
        "print(remove_id_list)"
      ],
      "metadata": {
        "id": "u-99RvAy57yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#불용어 제거\n",
        "f = open('/content/스탑워드 + 하다.txt', \"rt\", encoding=\"utf-8\") \n",
        "lines = f.readlines()\n",
        "stopwords = []\n",
        "unique_Noun_words=[]\n",
        "for line in lines:\n",
        "    line = line.replace('\\n', '')\n",
        "    stopwords.append(line)\n",
        "f.close()\n",
        "print(stopwords)\n",
        "\n",
        "\n",
        "unique_Noun_words=[]\n",
        "for i in range(len(remove_id_list)):#이차원 배열 전체에서 for 문 돌리기\n",
        "  tmp=[]\n",
        "  for j in range(len(remove_id_list[i])):#배열 안에서 일차원 배열의 길이가 서로 다 다르니깐 그 길이만큼 돌리면서\n",
        "    if remove_id_list[i][j] not in stopwords:#불용어 있는지 검사 없으면 tmp에 임시저장\n",
        "      tmp.append(remove_id_list[i][j])\n",
        "  unique_Noun_words.append(tmp)#tmp에 임시저장한 리스트를 배열 전체 하나가 끝날때마다 검사\n",
        "for i in range(len(unique_Noun_words)):\n",
        "  print(unique_Noun_words[i])"
      ],
      "metadata": {
        "id": "XPchHT6gSFtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = list(unique_Noun_words[:1000])\n",
        "test = []\n",
        "for doc in sample:\n",
        "    for word in doc:\n",
        "    \t# 특수문자, 숫자로만 구성된 단어, 1글자 단어\n",
        "        if word.isalnum() == False or word.isdigit() == True or len(word) == 1:\n",
        "#             print(\"특수\")\n",
        "            continue\n",
        "        test.append(word)\n",
        "\n",
        "# 등장 단어리스트 추출\n",
        "vocab = list(set(test))\n",
        "vocab.sort()"
      ],
      "metadata": {
        "id": "4H4w8dzI57xJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tf(t, d):\n",
        "    return d.count(t)\n",
        "    \n",
        "N = len(sample)\n",
        "\n",
        "result = []\n",
        "for i in tqdm(range(N)): # 각 문서에 대해서 아래 명령을 수행\n",
        "    result.append([])\n",
        "    d = sample[i]\n",
        "    \n",
        "    for j in range(len(vocab)):\n",
        "        t = vocab[j]        \n",
        "        result[-1].append(tf(t, d))\n",
        "        \n",
        "tf_ = pd.DataFrame(result, columns = vocab)"
      ],
      "metadata": {
        "id": "t-JBdAkotfcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from math import log\n",
        "\n",
        "\n",
        "def idf(t):\n",
        "    df = 0\n",
        "    for doc in sample:\n",
        "        df += t in doc\n",
        "    return log(N/(df + 1))\n",
        "\n",
        "\n",
        "result = []\n",
        "for j in range(len(vocab)):\n",
        "    t = vocab[j]\n",
        "    result.append(idf(t))\n",
        "    \n",
        "\n",
        "idf_ = pd.DataFrame(result, index = vocab, columns = [\"IDF\"])"
      ],
      "metadata": {
        "id": "m-RUM2DV57vQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tfidf(t, d):\n",
        "    return tf(t,d)* idf(t)\n",
        "\n",
        "result = []\n",
        "for i in tqdm(range(N)):\n",
        "    result.append([])\n",
        "    d = remove_id_list[i]\n",
        "    for j in range(len(vocab)):\n",
        "        t = vocab[j]\n",
        "\n",
        "        result[-1].append(tfidf(t,d))"
      ],
      "metadata": {
        "id": "xrC-WBditlJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_ = pd.DataFrame(result, columns = vocab)"
      ],
      "metadata": {
        "id": "ywSdt_w4wss9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_.sum().sort_values(ascending = False)[:50]"
      ],
      "metadata": {
        "id": "fblpL8meviaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idf_.sort_values(by = 'IDF', ascending = False)[:50]"
      ],
      "metadata": {
        "id": "aJfs0FXywHz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_.sum().sort_values(ascending = False)[:50]\n",
        "tfidfsum = pd.DataFrame(tfidf_.sum().sort_values(ascending = False)[:50])\n",
        "tfidfsum.head(10)\n",
        "weightlist = []\n",
        "for x in range(len(tfidfsum)):\n",
        "  weightlist.append(tfidfsum.index[x])\n",
        "weightdata = tfidfsum.to_dict('index')\n",
        "print(weightdata)"
      ],
      "metadata": {
        "id": "dKm3S9ikwRGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weightdict = {}\n",
        "for x in weightlist:\n",
        "  weightdict[x] = weightdata[x][0]\n",
        "print(weightdict)"
      ],
      "metadata": {
        "id": "qF0GKVUYDjeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "!apt-get update -qq\n",
        "!apt-get install fonts-nanum* -qq\n",
        "\n",
        "font = '/usr/share/fonts/truetype/nanum/NanumGothicEco.ttf'\n",
        "wordcloud = WordCloud(font_path = font, background_color='white',colormap = \"Accent_r\", \n",
        "                      width=3000, height=2000).generate_from_frequencies(weightdict)\n",
        "\n",
        "plt.imshow(wordcloud) \n",
        "plt.axis('off') \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "O29ifKaC5fvQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}